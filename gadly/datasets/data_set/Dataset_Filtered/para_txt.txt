# Prerequisites
# pip install nltk
# pip install pattern

# import nltk
# nltk.download('averaged_perceptron_tagger')
# nltk.download('punkt')
# nltk.download('stopwords')
# nltk.download('wordnet')
# nltk.download('omw-1.4')

import nltk
import random
import pandas as pd
import numpy as np
import spacy

from nltk.tokenize import word_tokenize, TreebankWordDetokenizer
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from nltk.tag import pos_tag
from pattern.en import pluralize, singularize

from transformers import GenerationMixin, TFGenerationMixin, PegasusForConditionalGeneration, PegasusTokenizerFast

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from gensim.models import KeyedVectors
from joblib import dump, load

from .models import Dataset, Dataset_Filtered

# from .utils import load_word2vec_model

class ML():
    def __init__(self):
        model_path = r'C:\Users\Chester Martinez\OneDrive\Documents\School\App Dev\CapstoneProj\gadly\backend\GoogleNews-vectors-negative300.bin'
        self.word_vectors = KeyedVectors.load_word2vec_format(model_path, binary=True, limit=800000)

        try: 
            self.gender_vectorizer = load(r'C:\Users\Chester Martinez\OneDrive\Documents\School\App Dev\CapstoneProj\gadly\backend/joblib/gender_vectorizer.joblib')
            self.gender_clf = load(r'C:\Users\Chester Martinez\OneDrive\Documents\School\App Dev\CapstoneProj\gadly\backend/joblib/gender_clf.joblib')
        except FileNotFoundError:
            self.gender_vectorizer, self.gender_clf = self.train_gender() 
            
        try: 
            self.sensitivity_vectorizer = load(r'C:\Users\Chester Martinez\OneDrive\Documents\School\App Dev\CapstoneProj\gadly\backend/joblib/sensitivity_vectorizer.joblib')
            self.sensitivity_clf = load(r'C:\Users\Chester Martinez\OneDrive\Documents\School\App Dev\CapstoneProj\gadly\backend/joblib/sensitivity_clf.jobli')
        except FileNotFoundError:
            self.sensitivity_vectorizer, self.sensitivity_clf = self.train_sensitivity() 
            
        # self.gender_vectorizer, self.gender_clf = self.train_gender() 
        # self.sensitivity_vectorizer, self.sensitivity_clf = self.train_sensitivity() 
        
    def train_gender(self):
        dataset = Dataset_Filtered.objects.exclude(gender='neutral')
        words = []
        labels = []
        
        for data in dataset:
            words.append(data.word)
            labels.append(data.gender)
            
        suffixes = [word[-3:] for word in words]
        features = [f"{word} {suffix}" for word, suffix in zip(words, suffixes)]

        gender_vectorizer = CountVectorizer()
        X = gender_vectorizer.fit_transform(features)
        X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)

        gender_clf = LogisticRegression()
        gender_clf.fit(X_train, y_train)
        accuracy = gender_clf.score(X_test, y_test)
        print("Accuracy:", accuracy)

        dump(gender_vectorizer, r'C:\Users\Chester Martinez\OneDrive\Documents\School\App Dev\CapstoneProj\gadly\backend/joblib/gender_vectorizer.joblib')
        dump(gender_clf, r'C:\Users\Chester Martinez\OneDrive\Documents\School\App Dev\CapstoneProj\gadly\backend/joblib/gender_clf.joblib') 
        return gender_vectorizer, gender_clf
    
    def train_sensitivity(self):
        dataset = Dataset_Filtered.objects.all()
        words = []
        labels = []
        # vect = [self.word_vectors[data.word] for data in dataset if data.word in self.word_vectors]
        # print(vect)
        
        for data in dataset:
            words.append(data.word)
            if data.gender == 'male' or data.gender == 'female':
                labels.append('gen_sen')    
            else:
                labels.append('not_gen_sen')
        
        suffixes = [word[-3:] for word in words]
        features = [f"{word} {suffix}" for word, suffix in zip(words, suffixes)]
        # print(features)
        
        sensitivity_vectorizer = CountVectorizer()
        X = sensitivity_vectorizer.fit_transform(features)
        X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)

        sensitivity_clf = LogisticRegression()
        sensitivity_clf.fit(X_train, y_train)
        accuracy = sensitivity_clf.score(X_test, y_test)
        print("Accuracy:", accuracy)

        dump(sensitivity_vectorizer, r'C:\Users\Chester Martinez\OneDrive\Documents\School\App Dev\CapstoneProj\gadly\backend/joblib/sensitivity_vectorizer.joblib')
        dump(sensitivity_clf, r'C:\Users\Chester Martinez\OneDrive\Documents\School\App Dev\CapstoneProj\gadly\backend/joblib/sensitivity_clf.joblib') 
        return sensitivity_vectorizer, sensitivity_clf
    
    def classify_gender(self, word):
        suffix = word[-3:]
        feature = f"{word} {suffix}"

        X_new = self.gender_vectorizer.transform([feature])
        pred = self.gender_clf.predict(X_new)[0]
        return pred
    
    def classify_sensitivity(self, word):
        suffix = word[-3:]
        feature = f"{word} {suffix}"

        X_new = self.sensitivity_vectorizer.transform([feature])
        pred = self.sensitivity_clf.predict(X_new)[0]
        return pred

class Para_txt():
    def filter_words(self, txt):
        nostop_words = []
        nouns = []
        words_dict = {}
        count = 0
        ml = ML()
        nlp = spacy.load('en_core_web_sm')
        
        doc = nlp(txt)
        words = word_tokenize(txt)
        stop_words = set(stopwords.words("english"))
        ents = [ent.text for ent in doc.ents]
        
        for word in words:
            if word.lower() not in stop_words:
                nostop_words.append(word)        
        tagged_words = pos_tag(nostop_words)
    
        for word,tag in tagged_words:
            if tag.startswith('NN') or tag == 'JJ':
                nouns.append(word)
                
        for word in nouns:
            if word not in ents and ml.classify_sensitivity(word) == 'gen_sen':
                gender = ml.classify_gender(word)
                if gender == 'male' or gender == 'female':
                    words_dict[count] = {word: [], 'gender': gender}
                    count += 1
        return words_dict, words
    

    def is_plural(self, word):
        wnl = WordNetLemmatizer()
        lemma = wnl.lemmatize(word, 'n')
        return True if word is not lemma else False
    
    
    def get_synonyms(self, word, pref):
        syns = []
        ml = ML()

        for wn in wordnet.synsets(word):
            for syn in wn.lemmas():
                if (ml.classify_sensitivity(syn.name()) == 'not_gen_sen'):
                    if self.is_plural(word):
                        syns.append(pluralize(syn.name()))
                    elif not self.is_plural(word):
                        syns.append(syn.name())

        for det, rep in pref.items():
            if rep in syns:
                syns.insert(0, rep)
        syns = list(dict.fromkeys(syns))
        # print(f"Synonyms: {syns}")
        return syns


    def filter_synonyms(self, words_dict, pref):
        rem_list=[]
        # print(f"word dict: {words_dict}")
        count = 0
        
        for ind, ent in words_dict.items():
            # print(f"ind: {ind}, ent: {ent}")
            for key, val in ent.items():
                if key != 'gender':
                    det = key
                    words_dict[ind][det] = self.get_synonyms(det, pref)
                    if len(words_dict[ind][det]) == 0:
                        rem_list.append(ind)
            count += 1
            
        for ind in rem_list:
            del words_dict[ind]
        # print(f"Words Dictionary: {words_dict}")
        return words_dict

    def replace_words(self, words, words_dict):
        for ind, ent in words_dict.items():
            for det, rep in ent.items():
                if rep != []:
                    words = list(map(lambda x: x.replace(det, rep[0]), words))
        sen = TreebankWordDetokenizer().detokenize(words)
        return words, sen


    def paraphrase(self, sentence, num_return_sequences=2, num_beams=2):
        model = PegasusForConditionalGeneration.from_pretrained("tuner007/pegasus_paraphrase")
        tokenizer = PegasusTokenizerFast.from_pretrained("tuner007/pegasus_paraphrase")

        input = tokenizer([sentence], truncation=True, padding="longest", return_tensors="pt")
        
        output = model.generate(
            **input,
            num_beams=num_beams,
            num_return_sequences=num_return_sequences,
        )
        output = tokenizer.batch_decode(output, skip_special_tokens=True)

        return output
    
    
    def para_txt(self, txt, pref={}):
        words_dict = {}
        words_data = {'dets': [], 'reps': [], 'syns': [], 'rep_dict': {}}

        words_dict, words = self.filter_words(txt)
        words_dict = self.filter_synonyms(words_dict, pref)
        words, sen = self.replace_words(words, words_dict)
        # print(words_dict)
       
        for ind, ent in words_dict.items():
            # print(f"ent:{ent}")
            for key, val in ent.items():
                if key != 'gender':
                    words_data['dets'].append(key)
                    words_data['reps'].append(val[0])
                    words_data['syns'].append(val)
                    words_data['rep_dict'][key] = val[0]
                
        # print(words_data)
        return words_dict, words_data, words, sen
    

 
    
# ml = ML()
# pred = ml.classify_gender('chairman')
# print(pred)
# pred = ml.classify_sensitivity('chairman')
# print(pred)

# para = Para_txt()
# words_dict, words_data, words, sen = para.para_txt('policeman and freshman', pref={})
# print(f'Words Dictionary: {words_dict}')
# print(f'Data: {words_data}')
# print(f'Words: {words}')
# print(f'Sentence: {sen}')

# model = Word2Vec.load(r'C:\Users\Chester Martinez\OneDrive\Documents\School\App Dev\CapstoneProj\gadly\backend\GoogleNews-vectors-negative300.bin')
# model.wv.similarity('france', 'spain')
# print(model)
